# ğŸ”¬ ResearchRank

> Mass-ranking academic CVs/resumes for research labs â€” terminal-native, LLM-powered, fully free.

![Python](https://img.shields.io/badge/Python-3.11+-blue)
![LLM](https://img.shields.io/badge/LLM-Groq%20%28free%29-orange)
![License](https://img.shields.io/badge/License-MIT-green)

---

## What it does

Feed it N resumes (PDF, DOCX, TXT, Markdown). Get back a ranked leaderboard with:

- **Structured extraction** â€” name, education, research roles, publications, skills, awards
- **Publication enrichment** â€” looks up each paper on Semantic Scholar (free API), fetches citation counts, open-access status, field of study
- **Venue ranking** â€” infers conference/journal rank (A\*, A, B, C) from a curated list of 100+ venues (NeurIPS, ICML, Nature, ACL, etc.) + heuristics
- **Publication timeline** â€” ASCII bar chart showing research activity per year
- **Multi-dimensional scoring** â€” Publications, Research Experience, Education, Role Fit, Trajectory
- **Rich terminal output** â€” leaderboard table, per-candidate deep-dive cards, score bars
- **JSON export** â€” machine-readable full results

---

## Quickstart

### 1. Get a free Groq API key

Sign up at [console.groq.com](https://console.groq.com) â€” free tier, no credit card needed.

```bash
export GROQ_API_KEY=gsk_your_key_here
```

### 2. Install

```bash
git clone https://github.com/your-lab/researchrank
cd researchrank
pip install -r requirements.txt
```

### 3. Run

```bash
# Basic â€” rank all CVs in a folder
python main.py --resumes ./cvs/

# With a role description (enables Fit scoring)
python main.py --resumes ./cvs/ --job "PhD researcher in NLP / LLMs with strong publication record"

# Show only top 5
python main.py --resumes ./cvs/ --top 5

# Custom scoring weights (must sum to 1.0)
python main.py --resumes ./cvs/ --weights "publications=0.5,research=0.3,education=0.1,fit=0.05,trajectory=0.05"

# Skip paper lookup (faster, offline-friendly)
python main.py --resumes ./cvs/ --no-papers

# Export full results to JSON
python main.py --resumes ./cvs/ --export results.json

# Verbose mode (see each step)
python main.py --resumes ./cvs/ --verbose
```

---

## Scoring System

| Dimension | Default Weight | What it measures |
|-----------|---------------|-----------------|
| **Publications** | 30% | Venue rank (A\*/A/B/C), citation count, first-authorship, paper count |
| **Research** | 25% | Quality & duration of research roles, institution prestige (MIT, DeepMind, etc.) |
| **Education** | 20% | Degree level (PhD > MS > BS), institution tier |
| **Role Fit** | 15% | Keyword alignment with job description + LLM assessment |
| **Trajectory** | 10% | Recency, career progression, awards |

All dimensions score 0â€“100. Final score is a weighted sum (0â€“100).

### Venue Rankings

Built from CORE, Scimago, and curated CS/ML conference lists:

| Rank | Examples |
|------|---------|
| **A\*** | NeurIPS, ICML, ICLR, CVPR, ACL, Nature, Science, SIGIR, STOC |
| **A** | AAAI, IJCAI, AISTATS, JMLR, TPAMI, INTERSPEECH, ICSE |
| **B** | ICWSM, CIKM, BMVC, ICDM, WACV |
| **C** | Workshops, arXiv preprints |

---

## Supported Formats

| Format | Library |
|--------|---------|
| PDF | `pdfplumber` (recommended) or `pypdf` |
| DOCX / DOC | `python-docx` |
| TXT | built-in |
| Markdown | built-in |

---

## Free APIs Used

| Service | Purpose | Key needed? |
|---------|---------|------------|
| [Groq](https://console.groq.com) | LLM extraction (`llama-3.3-70b-versatile`) | âœ… Free signup |
| [Semantic Scholar](https://api.semanticscholar.org) | Paper lookup, citations | âŒ No |
| [OpenAlex](https://openalex.org) | Fallback paper data | âŒ No |

---

## Configuration

Edit `config.py` to customize:

- **LLM model** â€” default `llama-3.3-70b-versatile` (best free quality), fallback `llama3-8b-8192` (faster)
- **Default weights** â€” change scoring dimension weights globally
- **Institution tiers** â€” add your own tier-1/tier-2 universities
- **Venue rankings** â€” add domain-specific venues (bioinformatics, HCI, etc.)

---

## Project Structure

```
researchrank/
â”œâ”€â”€ main.py                     # CLI entrypoint
â”œâ”€â”€ config.py                   # All configuration
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ parsers/
â”‚   â””â”€â”€ resume_parser.py        # PDF/DOCX/TXT â†’ raw text
â”œâ”€â”€ analyzers/
â”‚   â”œâ”€â”€ llm_analyzer.py         # Groq LLM â†’ structured data
â”‚   â””â”€â”€ paper_analyzer.py       # Semantic Scholar lookup + venue ranking
â”œâ”€â”€ rankers/
â”‚   â””â”€â”€ scorer.py               # Multi-dimensional scoring engine
â””â”€â”€ reporters/
    â””â”€â”€ terminal_reporter.py    # Rich terminal output + JSON export
```

---

## Output Example

```
                    ğŸ† RESEARCH CANDIDATE LEADERBOARD

  # â”‚ Candidate          â”‚ Total â”‚ Score Bar                     â”‚ Papers â”‚ Top Venue â”‚ ...
 â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€
 ğŸ¥‡ â”‚ Jane Smith         â”‚  84.3 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚   9    â”‚  â˜… A*     â”‚ ...
 ğŸ¥ˆ â”‚ Alex Chen          â”‚  71.8 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚   5    â”‚  â˜… A*     â”‚ ...
 ğŸ¥‰ â”‚ Raj Patel          â”‚  68.2 â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  â”‚   7    â”‚  â—† A      â”‚ ...
```

---

## Notes

- Groq free tier has rate limits (~30 req/min). For large batches (30+ CVs), the tool automatically backs off and retries.
- Paper lookup uses a 1.2s delay between Semantic Scholar requests to stay within free rate limits.
- The tool is for **lab-internal ranking only** â€” treat LLM outputs as decision support, not ground truth.

---

## License

MIT
